
## Difference-in-Differences

The Difference-in-Differences (DiD) method is a widely used statistical approach to evaluate the causal effect of an intervention by comparing changes over time between a treatment group and a control group. In our School Feeding Program (SFP), this technique can help us measure how the introduction of the program influenced student attendance rates by comparing schools that implemented the program (treatment group) with those that did not (control group), both before and after the program began.

This approach leverages the assumption that, without the intervention, both groups would have followed similar trends over time. By examining the difference in attendance rate changes between these groups, we isolate the program’s impact while controlling for underlying trends that affect all schools.

In this scenario, you have two rounds of data on two groups of schools: one group that enrolled in the program, and another that did not. Remembering the case of the enrolled and non- enrolled groups, you realize that you cannot simply compare the average attendance rates of the two groups because of selection bias. Because you have data for two periods for each school in the sample, you can use those data to solve some of these challenges by comparing the change in attendance rates for the two groups, assuming that the change in the attendance rates of the non-enrolled group reflects what would have happened to the attendance of the enrolled group in the absence of the program. Note that it does not matter which way you calculate the double difference.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Loading of packages, data, and seed setting here
library(haven)
library(tidyverse)
library(broom)
library(estimatr)
library(fishmethods)
library(kableExtra)
library(MatchIt)
library(modelsummary)
library(pwr)
library(rddensity)
library(skimr)
library(texreg)
library(gtsummary)

# you can read directly from github here --
trans_df <- read_csv("https://raw.githubusercontent.com/vmandela99/blog_vickman/refs/heads/main/posts/M%26E_01_School_Feeding_Causal_inference_%26_Counterfactuals/school_feeding.csv")

theme_set(ggpubr::theme_pubclean())

# subset data to only "eligible" units
df_elig <- trans_df %>%
  filter(eligible == 1)
```



```{r warning=FALSE}
library(tidyverse)
library(estimatr)
out_did <- lm_robust(attendance_rate ~ round * enrolled, 
                     data = trans_df %>% filter(treatment_locality == 1),
                     clusters = locality_identifier)

out_did_wcov <- lm_robust(attendance_rate ~ round * enrolled +
                            age_hh + age_sp + educ_hh + educ_sp + 
                            female_hh + indigenous + hhsize + dirtfloor + 
                            bathroom + land + school_distance, 
                          data = trans_df %>% filter(treatment_locality == 1),
                          clusters = locality_identifier)

tbl_did <- tbl_regression(out_did, intercept = T) %>% 
  add_glance_source_note(
    glance_fun = broom::glance, # Extract model summary
    include = c("adj.r.squared", "r.squared", "nobs") # Add Adjusted R-squared
  )

tbl_did_wcov <- tbl_regression(out_did_wcov, intercept = T) %>% 
  add_glance_source_note(
    glance_fun = broom::glance, # Extract model summary
    include = c("adj.r.squared", "r.squared", "nobs") # Add Adjusted R-squared
  )


tbl_merge_did <-
  tbl_merge(
    tbls = list( tbl_did, tbl_did_wcov),
    tab_spanner = c("**No Covariate Adjustment**", "**With Covariate Adjustment**")
  )

tbl_merge_did

```

Next, you estimate the effect using regression analysis. Using a simple linear regression to compute the simple difference-in- differences estimate, you find that the program increased school attendance rate by 7.0. You then refine your analysis by adding additional control variables. In other words, you use a multivariate linear regression that takes into account a host of other factors, and you find the same promotion in school attendance rate.

**What are the basic assumptions required to accept this result from difference-in-differences?**

To accept this result, we assume that there are no differential time varying factors between the two groups other than the program. We assume that the treatment and comparison groups would have equal trends or changes in outcomes in the absence of treatment. While this assumption can’t be tested in the post intervention period, we can compare trends before the intervention starts.

**Based on the result from difference-in-differences, should HISP be scaled up nationally?**

No, based on this result, the SFP should not be scaled up nationally because it has Increased by less than the \$10 threshold level. Taking the estimated impact under random assignment as the “true” impact of the program suggests that the difference in difference estimate may be biased. In fact, in this case, using the nonenrolled households as a comparison group does not accurately represent the counterfactual trend in attendance rates.
