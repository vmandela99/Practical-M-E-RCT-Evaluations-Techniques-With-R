[
  {
    "objectID": "Chapter1.html",
    "href": "Chapter1.html",
    "title": "1  Variable Definitions",
    "section": "",
    "text": "1.0.1 Outcome Variable\nThis variable measures the primary goal of the SFP: improving school attendance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Variable Definitions</span>"
    ]
  },
  {
    "objectID": "Chapter1.html#section",
    "href": "Chapter1.html#section",
    "title": "1  Variable Definitions",
    "section": "1.1 ",
    "text": "1.1",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Variable Definitions</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Cover picture courtesy of Blessman International\nDesigning impactful program evaluations is crucial for understanding the effectiveness of interventions in various domains, including health, education, and welfare. While methodologies often overlap, the specific design and variables must be tailored to the context of the program being evaluated. This blog post takes inspiration from a health program evaluation dataset to construct a similar design for evaluating the impact of a School Feeding Program (SFP).\nThe School Feeding Program aims to improve student outcomes by providing nutritious meals during school hours, potentially influencing attendance, retention, and overall well-being. By adapting the same evaluation framework used for health interventions, we can explore how carefully chosen variables capture the nuances of such educational programs. This blog post demonstrates how to replicate the evaluation framework by defining variables for outcomes, controls, and other program-specific characteristics, ensuring a one-to-one correspondence with the health program’s design. See Knuth (1984) for additional discussion of literate programming.\nThe R code to get the data is here:-\n\n# Loading of packages, data, and seed setting here\nlibrary(haven)\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(estimatr)\nlibrary(fishmethods)\nlibrary(kableExtra)\nlibrary(MatchIt)\nlibrary(modelsummary)\nlibrary(pwr)\nlibrary(rddensity)\nlibrary(skimr)\nlibrary(texreg)\nlibrary(gtsummary)\n\n# you can read directly from github here --\ntrans_df &lt;- read_csv(\"https://raw.githubusercontent.com/vmandela99/blog_vickman/refs/heads/main/posts/M%26E_01_School_Feeding_Causal_inference_%26_Counterfactuals/school_feeding.csv\")\n\ntheme_set(ggpubr::theme_pubclean())\n\n# subset data to only \"eligible\" units\ndf_elig &lt;- trans_df %&gt;%\n  filter(eligible == 1)\n\nWhether you are a researcher, program implementer, or enthusiast in Monitoring and Evaluation (M&E), this approach equips you with a transferable method for evaluating interventions across various sectors. Let’s delve into how this framework is tailored for the School Feeding Program evaluation.\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Chapter2.html",
    "href": "Chapter2.html",
    "title": "2  Causal Inference and Counterfactuals",
    "section": "",
    "text": "Causal inference is the process of determining whether a program or intervention (like the School Feeding Program) directly causes a change in an outcome, such as improved school attendance. It goes beyond simple associations to uncover cause-and-effect relationships by comparing what happened with what could have happened if the program had not been implemented.\nThe idea of counterfactuals lies at the heart of causal inference. A counterfactual refers to the hypothetical scenario of what would have occurred in the absence of the program. Since we cannot observe both the actual outcome and the counterfactual for the same household, researchers rely on rigorous study designs (such as randomized control trials) or statistical techniques to estimate the counterfactual and isolate the program’s impact.\n\n2.0.1 Before-After Designs\nThe first “expert” consultant you hire suggests that to estimate the impact of the School Feeding Program (SFP), you should calculate the change in student attendance rates over time for the schools where households enrolled in the program. The consultant argues that because SFP provides meals that alleviate hunger and improve student focus, any increase in attendance rates over time can be attributed to the program’s effect.\nUsing the subset of schools in treatment localities, you calculate their average student attendance rates before the implementation of the program and then again two years later. The analysis focuses on comparing the average attendance rates at baseline and follow-up to assess the program’s impact in villages participating in the School Feeding Program.\n\nm_ba1 &lt;- lm_robust(attendance_rate ~ round, \n                   clusters = locality_identifier,\n                   data = trans_df %&gt;% filter(treatment_locality==1 & enrolled ==1))\n\n\nm_ba2 &lt;- lm_robust(attendance_rate ~ round + age_hh + age_sp + educ_hh + \n                     educ_sp + female_hh + indigenous + hhsize + dirtfloor + \n                     bathroom + land + school_distance, \n                   clusters = locality_identifier,\n                   data = trans_df %&gt;% filter(treatment_locality==1 & enrolled ==1))\n\nt0 &lt;- tbl_regression(m_ba1, intercept = T)\nt01 &lt;- tbl_regression(m_ba2, intercept = T)\n\ntbl_merge_m_ba &lt;-\n  tbl_merge(\n    tbls = list(t0, t01),\n    tab_spanner = c(\"No Controls\", \"With Controls\")\n  )\n\ntbl_merge_m_ba\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo Controls\n\n\nWith Controls\n\n\n\nBeta\n95% CI1\np-value\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n88\n87, 88\n&lt;0.001\n79\n78, 80\n&lt;0.001\n\n\nround\n5.7\n5.3, 6.1\n&lt;0.001\n5.7\n5.3, 6.1\n&lt;0.001\n\n\nage_hh\n\n\n\n\n\n\n-0.05\n-0.07, -0.03\n&lt;0.001\n\n\nage_sp\n\n\n\n\n\n\n0.00\n-0.03, 0.02\n&gt;0.9\n\n\neduc_hh\n\n\n\n\n\n\n-0.05\n-0.11, 0.01\n0.079\n\n\neduc_sp\n\n\n\n\n\n\n0.07\n0.00, 0.13\n0.038\n\n\nfemale_hh\n\n\n\n\n\n\n-0.86\n-1.4, -0.28\n0.004\n\n\nindigenous\n\n\n\n\n\n\n1.7\n1.3, 2.1\n&lt;0.001\n\n\nhhsize\n\n\n\n\n\n\n1.5\n1.5, 1.6\n&lt;0.001\n\n\ndirtfloor\n\n\n\n\n\n\n2.0\n1.7, 2.3\n&lt;0.001\n\n\nbathroom\n\n\n\n\n\n\n-0.31\n-0.60, -0.02\n0.039\n\n\nland\n\n\n\n\n\n\n-0.08\n-0.13, -0.04\n&lt;0.001\n\n\nschool_distance\n\n\n\n\n\n\n0.00\n0.00, 0.01\n0.14\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nDoes the before-and-after comparison control for all the factors that affect student attendance over time?\nNo, it is unlikely that this analysis accounts for all the factors influencing attendance. For instance, there could be other educational or health-related interventions occurring simultaneously in the communities receiving the School Feeding Program (SFP), which might also contribute to changes in attendance. Additionally, external factors like a regional economic crisis or natural disasters could have independently affected attendance. In the absence of SFP, attendance might have increased or decreased due to these factors, making it challenging to attribute all observed changes solely to the program.\nBased on these results produced by the before-and-after analysis, should SFP be scaled up nationally?\nNo, based on the current results, scaling up the program nationally might not be justified yet. While the School Feeding Program appears to have improved average attendance rates, the increase of 5.7 percentage points may not be sufficient to meet the government’s threshold for program effectiveness. Moreover, without understanding the contribution of confounding factors, it remains unclear whether the observed improvements are entirely due to the program.\n\n\n2.0.2 Enrolled vs. Non-Enrolled\nAnother consultant proposes a different approach, suggesting it would be more appropriate to estimate the counterfactual in the post-intervention period, two years after the program’s start. The consultant correctly notes that of the 5,929 households in the baseline sample, only 2,907 enrolled in the School Feeding Program (SFP), leaving approximately 51 percent of households without access to SFP.\nThe consultant argues that all schools within the 100 pilot villages were eligible to enroll in the program, with households in these communities sharing similar characteristics. For example, households rely on comparable school infrastructures, face similar regional conditions, and have children subject to the same school policies. Furthermore, economic activities and living standards within these localities are generally uniform.\nThe consultant asserts that under such circumstances, attendance rates for households not enrolled in SFP after the intervention can reasonably estimate the counterfactual outcomes for those enrolled. Consequently, you decide to compare average student attendance rates in the post-intervention period for both groups—schools participating in the School Feeding Program and those that opted out.\n\nm_ene1 &lt;- lm_robust(attendance_rate ~ enrolled, \n                    clusters = locality_identifier,\n                    data = trans_df %&gt;% filter(treatment_locality==1 & round ==1))\n\nm_ene2 &lt;- lm_robust(attendance_rate ~ enrolled + age_hh + age_sp + educ_hh + \n                      educ_sp + female_hh + indigenous + hhsize + dirtfloor + \n                      bathroom + land + school_distance, \n                    clusters = locality_identifier,\n                    data = trans_df %&gt;% filter(treatment_locality==1 & round ==1))\n\nt0a &lt;- tbl_regression(m_ene1, intercept = T)\nt0a1 &lt;- tbl_regression(m_ene2, intercept = T)\n\ntbl_merge_m_ene &lt;-\n  tbl_merge(\n    tbls = list(t0a, t0a1),\n    tab_spanner = c(\"No Controls\", \"With Controls\")\n  )\n\ntbl_merge_m_ene\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo Controls\n\n\nWith Controls\n\n\n\nBeta\n95% CI1\np-value\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n81\n80, 81\n&lt;0.001\n74\n72, 76\n&lt;0.001\n\n\nenrolled\n12\n12, 13\n&lt;0.001\n8.5\n8.0, 9.1\n&lt;0.001\n\n\nage_hh\n\n\n\n\n\n\n-0.08\n-0.12, -0.04\n&lt;0.001\n\n\nage_sp\n\n\n\n\n\n\n0.04\n0.00, 0.09\n0.045\n\n\neduc_hh\n\n\n\n\n\n\n0.04\n-0.06, 0.14\n0.5\n\n\neduc_sp\n\n\n\n\n\n\n0.10\n-0.02, 0.22\n0.088\n\n\nfemale_hh\n\n\n\n\n\n\n-0.50\n-1.6, 0.58\n0.4\n\n\nindigenous\n\n\n\n\n\n\n1.7\n0.84, 2.5\n&lt;0.001\n\n\nhhsize\n\n\n\n\n\n\n1.7\n1.6, 1.8\n&lt;0.001\n\n\ndirtfloor\n\n\n\n\n\n\n1.7\n1.2, 2.3\n&lt;0.001\n\n\nbathroom\n\n\n\n\n\n\n-0.53\n-1.1, 0.00\n0.052\n\n\nland\n\n\n\n\n\n\n0.01\n-0.09, 0.11\n0.8\n\n\nschool_distance\n\n\n\n\n\n\n0.01\n-0.01, 0.02\n0.3\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nDoes this analysis likely control for all the factors that determine differences in student attendance between the enrolled and non-enrolled groups?\nNo, it is unlikely that the multivariate analysis fully controls for all the factors that influence the difference in attendance rates between the two groups. There could be unobservable factors that contribute to why some schools enroll in the feeding program while others do not. For instance, household preferences, school engagement levels, or the motivation of parents could play a role in determining which schools opt for the program. These factors may not be fully captured in the analysis.\nBased on these results produced by the enrolled vs. non-enrolled method, should the School Feeding Program (SFP) be scaled up nationally?\nBased strictly on the estimate from the multivariate linear regression, the SFP should not be scaled up nationally based on the findings here. The program increased average student attendance by 8.5%, which is a positive but modest improvement. While this result is statistically significant (p-value &lt; 0.001), it is lower than the expected national threshold improvement in attendance, suggesting that scaling up the program may not immediately achieve the desired outcomes at a larger scale. However, the modest effect size means that further investigation into the program’s impact across different contexts and regions is necessary to determine if it could contribute meaningfully to national efforts in improving student attendance.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Causal Inference and Counterfactuals</span>"
    ]
  },
  {
    "objectID": "Chapter3.html",
    "href": "Chapter3.html",
    "title": "3  Randomized Assignment",
    "section": "",
    "text": "Random Assignment in the context of our school feeding program evaluation means that households or communities are randomly assigned to either the treatment group (where they receive the feeding program) or the control group (where they do not). This random assignment ensures that every participant has an equal chance of being placed in either group, making it more likely that the groups are similar at the start of the study. As a result, any differences in outcomes, such as changes in children’s health or learning outcomes, can be attributed to the school feeding program itself, rather than other external factors. This method strengthens the validity of our findings and helps ensure that the observed impacts are genuinely due to the program intervention.\nThe key is to find a group of villages that are very similar to the 100 treatment villages, except for the fact that one group participated in the school feeding program and the other did not. Since the treatment villages were randomly selected from the pool of rural villages, they should, on average, have similar characteristics to those villages that did not participate in the program.\nTo improve the counterfactual estimate, we utilize an additional 100 rural villages that were not part of the feeding program. These comparison villages were also randomly selected, ensuring that they share similar characteristics with the treatment villages at the outset of the program. The random assignment of the program ensures that any differences in outcomes (e.g., improvements in children’s nutrition or learning) between the treatment and comparison villages can be attributed to the program, not external factors.\nTo validate this assumption, we would need to test whether the characteristics of eligible households in both the treatment and comparison villages were similar at the baseline, ensuring that no major differences existed before the program began. If the characteristics are similar, it further supports the idea that the program’s effects are due to the intervention itself rather than other external factors.\n\ndf_elig &lt;- trans_df %&gt;%\n  filter(eligible == 1) \n\ndf_elig %&gt;% \n  filter(round == 0) %&gt;%\n  dplyr::select(treatment_locality, locality_identifier,\n                age_hh, age_sp, educ_hh, educ_sp, female_hh, indigenous, \n                hhsize, dirtfloor, bathroom, land, school_distance) %&gt;%\n  tidyr::pivot_longer(-c(\"treatment_locality\",\"locality_identifier\")) %&gt;%\n  group_by(name) %&gt;%\n  do(tidy(lm_robust(value ~ treatment_locality, data = .))) %&gt;%\n  filter(term == \"treatment_locality\") %&gt;%\n  dplyr::select(name, estimate, std.error, p.value) %&gt;%\n  kable()\n\n\n\n\nname\nestimate\nstd.error\np.value\n\n\n\n\nage_hh\n-0.6354625\n0.3759583\n0.0910361\n\n\nage_sp\n-0.0386302\n0.3120790\n0.9014911\n\n\nbathroom\n0.0149907\n0.0132340\n0.2573724\n\n\ndirtfloor\n-0.0129497\n0.0118744\n0.2755159\n\n\neduc_hh\n0.1607976\n0.0697576\n0.0211978\n\n\neduc_sp\n0.0289107\n0.0670018\n0.6661273\n\n\nfemale_hh\n-0.0041155\n0.0070493\n0.5593691\n\n\nhhsize\n0.0596953\n0.0530454\n0.2604833\n\n\nindigenous\n0.0091048\n0.0131969\n0.4902756\n\n\nland\n-0.0402168\n0.0704607\n0.5681787\n\n\nschool_distance\n2.9087631\n1.1323148\n0.0102288\n\n\n\n\n\nThe average characteristics of households in both the treatment and comparison villages appear very similar. Among the various variables tested, the only statistically significant differences are in the number of years of education of the head of household and the distance to the nearest school, which are relatively small in magnitude. Specifically, the difference in the education of the household head is 0.16 years (which is less than 6% of the average years of education in the comparison group), and the difference in the distance to school is 2.91 kilometers (less than 3% of the comparison group’s average distance). These differences are statistically significant, but small, indicating that the two groups are quite similar in terms of key demographic factors.\nEven in a randomized experiment involving a large sample, small differences can occur by chance due to the nature of statistical tests. In fact, using a typical 5% significance level, we would expect some differences in around 5% of the characteristics simply due to random variability. Therefore, although small statistically significant differences exist, the overall similarity between the two groups suggests that the random assignment was effective and that the treatment and comparison groups are comparable for the evaluation of the feeding program’s impact.\nEstimate the average attendance rate for eligible households in the treatment and comparison villages for each period. What is the impact of the program?\n\nout_round0 &lt;- lm_robust(attendance_rate ~ treatment_locality,\n                        data = df_elig %&gt;% filter(round == 0),\n                        clusters = locality_identifier)\nout_round1 &lt;- lm_robust(attendance_rate ~ treatment_locality,\n                        data = df_elig %&gt;% filter(round == 1),\n                        clusters = locality_identifier)\n\nt0b &lt;- tbl_regression(out_round0, intercept = T)\nt0b1 &lt;- tbl_regression(out_round1, intercept = T)\n\ntbl_merge_m_ba1 &lt;-\n  tbl_merge(\n    tbls = list(t0b, t0b1),\n    tab_spanner = c(\"Baseline\", \"Follow Up\")\n  )\n\ntbl_merge_m_ba1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nBaseline\n\n\nFollow Up\n\n\n\nBeta\n95% CI1\np-value\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n88\n87, 88\n&lt;0.001\n85\n84, 85\n&lt;0.001\n\n\ntreatment_locality\n0.07\n-0.29, 0.44\n0.7\n8.7\n8.0, 9.4\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nAt baseline, there is no statistically significant difference in the average characteristics between the treatment and comparison groups. This confirms that the groups are similar, as expected under randomized assignment. The baseline results show that the treatment locality (the area receiving the feeding program) does not significantly differ from the comparison group in terms of the outcome measure (Beta = 0.07, p-value = 0.7).\nAt follow-up, however, the treatment locality shows a statistically significant and positive effect on the outcome measure, with a beta coefficient of 8.7 (p-value &lt; 0.001). This indicates that households in the treatment locality saw a notable improvement compared to those in the comparison villages. Specifically, the intervention appears to have resulted in an increase in the outcome, possibly reflecting the positive effects of the feeding program, given the substantial change in the beta coefficient.\nThe impact of the program is therefore evident in the follow-up period, and the reduction in the treatment and comparison villages’ differences shows a clear program effect, with an estimated increase of 8.7 units on the attendance rate, which is statistically significant.\nThus, these findings support the conclusion that the feeding program had a positive impact on the target population over the course of the study period.\nRe-estimate using a multivariate regression analysis that controls for the other observable characteristics of the sample households. How does your impact estimate change?\n\nout_round1_nocov &lt;- lm_robust(attendance_rate ~ treatment_locality,\n                              data = df_elig %&gt;% filter(round == 1),\n                              clusters = locality_identifier)\nout_round1_wcov &lt;- lm_robust(attendance_rate ~ treatment_locality +\n                               age_hh + age_sp + educ_hh + educ_sp + \n                               female_hh + indigenous + hhsize + dirtfloor + \n                               bathroom + land + school_distance,\n                             data = df_elig %&gt;% filter(round == 1),\n                             clusters = locality_identifier)\nt2 &lt;- tbl_regression(out_round1_nocov, intercept = T)\nt3 &lt;- tbl_regression(out_round1_wcov, intercept = T)\n\ntbl_merge_out_round1 &lt;-\n  tbl_merge(\n    tbls = list(t2, t3),\n    tab_spanner = c(\"**No Covariate Adjust.**\", \"**With Covariate Adjust.**\")\n  )\n\ntbl_merge_out_round1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo Covariate Adjust.\n\n\nWith Covariate Adjust.\n\n\n\nBeta\n95% CI1\np-value\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n85\n84, 85\n&lt;0.001\n76\n75, 78\n&lt;0.001\n\n\ntreatment_locality\n8.7\n8.0, 9.4\n&lt;0.001\n8.6\n8.0, 9.2\n&lt;0.001\n\n\nage_hh\n\n\n\n\n\n\n-0.04\n-0.06, -0.01\n0.006\n\n\nage_sp\n\n\n\n\n\n\n0.00\n-0.03, 0.03\n0.9\n\n\neduc_hh\n\n\n\n\n\n\n0.03\n-0.05, 0.11\n0.4\n\n\neduc_sp\n\n\n\n\n\n\n0.02\n-0.06, 0.10\n0.7\n\n\nfemale_hh\n\n\n\n\n\n\n-0.55\n-1.3, 0.21\n0.2\n\n\nindigenous\n\n\n\n\n\n\n1.6\n1.0, 2.2\n&lt;0.001\n\n\nhhsize\n\n\n\n\n\n\n1.4\n1.3, 1.5\n&lt;0.001\n\n\ndirtfloor\n\n\n\n\n\n\n1.6\n1.1, 2.1\n&lt;0.001\n\n\nbathroom\n\n\n\n\n\n\n-0.24\n-0.67, 0.18\n0.3\n\n\nland\n\n\n\n\n\n\n-0.03\n-0.10, 0.03\n0.3\n\n\nschool_distance\n\n\n\n\n\n\n0.00\n-0.01, 0.01\n0.5\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nWithout Covariate Adjustment:\nAt the baseline, the coefficient for the treatment locality is 8.7 (with a 95% confidence interval of 8.0 to 9.4), which is statistically significant (p-value &lt; 0.001). This suggests that, without adjusting for other factors, the households in the treatment locality (those receiving the feeding program) exhibit a significant improvement compared to those in the comparison group.\nWith Covariate Adjustment:\nWhen adjusting for other observable characteristics such as age, education, and household size, the coefficient for the treatment locality is slightly reduced to 8.6 (95% CI: 8.0 to 9.2) but remains statistically significant (p-value &lt; 0.001). This indicates that even when accounting for factors like age, education, and household characteristics, the treatment locality still shows a strong and positive effect, with the intervention leading to a substantial improvement in the outcome measure.\n\n3.0.1 Why is the Impact Estimate Unchanged with Covariate Adjustment?\nThe treatment effect remains nearly unchanged when controlling for additional factors because of the randomized assignment. Randomization ensures that the treatment and comparison groups are very similar in characteristics at baseline, and external factors affecting the outcome should affect both groups equally over time. Therefore, any changes observed in the treatment locality compared to the comparison group can confidently be attributed to the feeding program rather than differences in baseline characteristics or external influences.\n\n\n3.0.2 Conclusion on the Program’s Impact\nGiven that the estimated impact remains consistent even after controlling for additional characteristics, it is clear that the feeding program has a significant positive effect on the target population. The treatment group shows a noticeable improvement in outcomes, and this improvement is robust to covariate adjustments.\n\n\n3.0.3 Should the Feeding Program Be Scaled Up?\nYes, the feeding program should be scaled up. The impact on the outcome measure is statistically significant and substantial. The effect of the intervention, even after accounting for other factors, supports the case for expanding the program to other regions to improve the well-being of households in similar circumstances.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Randomized Assignment</span>"
    ]
  },
  {
    "objectID": "Chapter4.html",
    "href": "Chapter4.html",
    "title": "4  Instrumental Variables",
    "section": "",
    "text": "Instrumental Variables in the context of our school feeding program help us figure out how the program affects attendance rates when other factors might confuse the results. An instrumental variable is something that influences whether a child participates in the program (like whether their school is in a treatment area) but doesn’t directly impact attendance rates except through the program itself. This approach helps isolate the program’s true effect on attendance, even if there are other overlapping influences.\nLet us now try using the randomized promotion method to evaluate the impact of the school feeding program (SFP) on attendance rates. Imagine the Ministry of Education decides that the feeding program should eventually be made available to all schools nationwide. This is a different situation from the randomized assignment design we’ve considered so far. However, given the logistical realities of scaling the program, you propose an incremental rollout.\nTo assess its impact, you randomly select a subset of schools (indicated by promotion_locality) to receive an intensive promotion campaign aimed at increasing awareness and participation in the feeding program. This campaign includes activities such as community outreach, parent meetings, and tailored communication materials to emphasize the program’s benefits. Importantly, the promotion focuses solely on raising awareness and boosting program enrollment, ensuring it does not directly encourage unrelated behaviors that could influence attendance rates. This design ensures the promotion can be used as a valid instrumental variable (IV) for understanding how the feeding program affects attendance rates.\nWhat was the effect of the promotion campaign upon enrollment?\nNote you should use the variable enrolled_rp for this question\n\nm_enroll &lt;- lm_robust(enrolled_rp ~ promotion_locality,\n                      clusters = locality_identifier,\n                      data = trans_df %&gt;% filter(round == 1))\n\nt_enroll &lt;- tbl_regression(m_enroll, intercept = TRUE) %&gt;%\n  add_glance_source_note(\n    glance_fun = broom::glance, # Extract model summary\n    include = c(\"adj.r.squared\", \"r.squared\", \"nobs\") # Add Adjusted R-squared\n  )\n\nt_enroll\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n0.08\n0.04, 0.13\n0.001\n    promotion_locality\n0.41\n0.34, 0.48\n&lt;0.001\n  \n  \n    \n      Adjusted R² = 0.200; R² = 0.200; No. Obs. = 9,914\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nAfter two years of promotion and program implementation, you find that 41% of students in schools randomly assigned to the promotion campaign are attending school more regularly, compared to only 8% in non-promoted schools.\nBecause the promoted and non-promoted schools were assigned at random, we can confidently assume that the baseline characteristics of the two groups were similar in the absence of the promotion. This assumption is validated by comparing baseline attendance rates and other school-related characteristics, which showed no significant differences.\nFrom the results in Table 2, the estimated impact of the promotion locality on attendance rates is a significant increase of 41 percentage points (Beta = 0.41, 95% CI [0.34, 0.48], p &lt; 0.001). The intercept (baseline attendance rate in non-promoted schools) was estimated at 8% (Beta = 0.08, 95% CI [0.04, 0.13], p = 0.001). The model explains 20% of the variation in attendance rates (Adjusted R² = 0.20), indicating a strong relationship between promotion and improved attendance outcomes.\nCompare baseline attendance rates based upon assignment to promotion.\n\nm_base_attend &lt;- lm_robust(attendance_rate ~ promotion_locality,\n                           clusters = locality_identifier,\n                           data = trans_df %&gt;% filter(round == 0)\n)\n\nt_base_attend &lt;- tbl_regression(m_base_attend, intercept = TRUE) %&gt;%\n  add_glance_source_note(\n    glance_fun = broom::glance, # Extract model summary\n    include = c(\"adj.r.squared\", \"r.squared\", \"nobs\") # Add Adjusted R-squared\n  ) \nt_base_attend\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n85\n85, 86\n&lt;0.001\n    promotion_locality\n0.05\n-0.44, 0.53\n0.9\n  \n  \n    \n      Adjusted R² = 0.000; R² = 0.000; No. Obs. = 9,913\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nEstimate the difference in attendance rates by assignment to promotion, in the post-treatment period\n\nm_post_attend &lt;- lm_robust(attendance_rate ~ promotion_locality,\n                           clusters = locality_identifier,\n                           data = trans_df %&gt;% filter(round == 1)\n)\n\nt_post_attend &lt;- tbl_regression(m_post_attend, intercept = TRUE) %&gt;%\n  add_glance_source_note(\n    glance_fun = broom::glance, # Extract model summary\n    include = c(\"adj.r.squared\", \"r.squared\", \"nobs\") # Add Adjusted R-squared\n  ) \nt_post_attend\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n84\n83, 85\n&lt;0.001\n    promotion_locality\n3.3\n2.2, 4.4\n&lt;0.001\n  \n  \n    \n      Adjusted R² = 0.026; R² = 0.027; No. Obs. = 9,914\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nUsing this attendance rate estimate and the estimated proportion of “compliers”, estimate the LATE/CACE\n\n4.0.1 LATE (Local Average Treatment Effect):\nLATE measures the effect of the program only on the group that complied with the treatment assignment (e.g., those who were offered the feeding program and actually participated). It tells us the impact on attendance rates for these participants, not for everyone assigned to the treatment or control groups.\n\n\n4.0.2 CACE (Complier Average Causal Effect):\nCACE is essentially the same as LATE in many contexts, particularly in randomized trials. It focuses on estimating the causal effect of the program for those who adhered to their assignment (e.g., those who were assigned to the treatment and participated, or those in the control who did not access the treatment).\n\nm_cace &lt;- iv_robust(attendance_rate ~ enrolled_rp |\n                      promotion_locality,\n                    clusters = locality_identifier,\n                    data = trans_df %&gt;% filter(round == 1))\n\nm_cace_wcov &lt;- iv_robust(attendance_rate ~ enrolled_rp + \n                           age_hh + age_sp + educ_hh + educ_sp + \n                           female_hh + indigenous + hhsize + dirtfloor + \n                           bathroom + land + school_distance | \n                           promotion_locality + \n                           age_hh + age_sp + educ_hh + educ_sp + \n                           female_hh + indigenous + hhsize + dirtfloor + \n                           bathroom + land + school_distance ,\n                         clusters = locality_identifier,\n                         data = trans_df %&gt;% filter(round == 1))\n\n\nt_cace &lt;- tbl_regression(m_cace, intercept = T)\nt_cace_wcov &lt;- tbl_regression(m_cace_wcov, intercept = T)\n\ntbl_merge_cace &lt;-\n  tbl_merge(\n    tbls = list(t_cace, t_cace_wcov),\n    tab_spanner = c(\"**No Covariate Adjust.**\", \"**With Covariate Adjust.**\")\n  )\n\ntbl_merge_cace\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nNo Covariate Adjust.\n\n\nWith Covariate Adjust.\n\n\n\nBeta\n95% CI1\np-value\nBeta\n95% CI1\np-value\n\n\n\n\n(Intercept)\n83\n82, 84\n&lt;0.001\n75\n74, 76\n&lt;0.001\n\n\nenrolled_rp\n8.1\n6.2, 10\n&lt;0.001\n8.3\n6.7, 10\n&lt;0.001\n\n\nage_hh\n\n\n\n\n\n\n-0.06\n-0.09, -0.04\n&lt;0.001\n\n\nage_sp\n\n\n\n\n\n\n0.01\n-0.02, 0.04\n0.4\n\n\neduc_hh\n\n\n\n\n\n\n-0.03\n-0.11, 0.04\n0.4\n\n\neduc_sp\n\n\n\n\n\n\n0.04\n-0.04, 0.12\n0.4\n\n\nfemale_hh\n\n\n\n\n\n\n-0.88\n-1.7, -0.11\n0.025\n\n\nindigenous\n\n\n\n\n\n\n2.0\n1.3, 2.7\n&lt;0.001\n\n\nhhsize\n\n\n\n\n\n\n1.7\n1.6, 1.9\n&lt;0.001\n\n\ndirtfloor\n\n\n\n\n\n\n1.8\n1.3, 2.3\n&lt;0.001\n\n\nbathroom\n\n\n\n\n\n\n-0.59\n-1.0, -0.18\n0.005\n\n\nland\n\n\n\n\n\n\n-0.08\n-0.16, 0.00\n0.039\n\n\nschool_distance\n\n\n\n\n\n\n0.00\n0.00, 0.01\n0.3\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nCompare baseline attendance rates based upon assignment to promotion\nThe baseline attendance rates show that there is no significant difference in attendance rates between schools assigned to the promotion (SFP) and those that were not, as indicated by the promotion locality coefficient of 0.05 with a 95% Confidence Interval (CI) of [-0.44, 0.53] and a p-value of 0.9. The p-value is greater than the 0.05 significance level, which means that any differences in baseline attendance are likely due to random variation, not the promotion itself.\nEstimate the difference in attendance rates by assignment to promotion, in the post-treatment period\nThe post attendance period shows a significant positive effect on attendance, with a coefficient of 3.3 (95% CI [2.2, 4.4], p-value &lt; 0.001). This suggests that schools assigned to the feeding program promotion saw an increase in attendance rates by about 3.3 percentage points compared to non-promoted schools. The model’s adjusted R² of 0.026 indicates that a small proportion of the variation in attendance is explained by the promotion, but the effect is still statistically significant.\nUsing this attendance rate estimate and the estimated proportion of “compliers,” estimate the LATE/CACE\nThe Local Average Treatment Effect (LATE) and the Complier Average Causal Effect (CACE) are estimated by considering both unadjusted and adjusted models.\nNo Covariate Adjustment: The coefficient for the promotion locality remains significant (Beta = 8.1, 95% CI [6.2, 10], p-value &lt; 0.001), suggesting that, on average, the promotion led to an 8.1 percentage point increase in attendance for those schools in the promotion locality who complied with the program.\nWith Covariate Adjustment: After adjusting for covariates such as household characteristics and school factors, the effect is slightly higher (Beta = 8.3, 95% CI [6.7, 10], p-value &lt; 0.001), reinforcing the robustness of the promotion’s impact. These results suggest that the school feeding program had a substantial positive impact on attendance rates, particularly for the compliers—those schools that adhered to the promotion. The covariate-adjusted estimates provide additional confidence that the observed effect is not solely driven by confounding factors.\nWhat are the key conditions for accepting the results from the randomized promotion evaluation of the SFP\n\nBaseline Equivalence: The schools in the promoted and non-promoted groups should have similar characteristics before the promotion. The baseline attendance rates in the first table show no significant difference, suggesting this assumption holds true.\nPromotion Effectiveness: The promotion should effectively increase school attendance. This assumption holds in the post-treatment period, where the promoted schools show a significant increase in attendance by 3.3 percentage points.\nNo Direct Effects on Other Factors: The promotion should only affect attendance and not other outcomes (such as student performance or health). This assumption cannot be directly tested but is informed by the program’s design focusing only on increasing school attendance.\n\nShould the SFP be Scaled Up Nationally?\nBased on the results from the regression analysis, the school feeding program shows a significant positive impact on attendance rates. The estimated LATE/CACE of 8.3 percentage points suggests a clear benefit from the program. Therefore, the results support scaling the program up nationally, as the program’s effectiveness in improving attendance is statistically significant and substantial.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Instrumental Variables</span>"
    ]
  },
  {
    "objectID": "Chapter5.html",
    "href": "Chapter5.html",
    "title": "5  Regression Discontinuity Designs",
    "section": "",
    "text": "Regression Discontinuity Designs (RDD) are a method used to evaluate the impact of a treatment or intervention by focusing on cases where a specific cutoff or threshold determines who receives the treatment. For example, imagine a school program that only allows students with test scores above a certain level to participate. RDD compares students who are just above the threshold (and get the program) with those who are just below it (and don’t get the program). This helps estimate the effect of the program by assuming that those on either side of the cutoff are very similar, except for receiving the program.\nIn simple terms, RDD looks at situations where a rule or score divides people into different groups and then compares those on either side of that line to see if the treatment makes a real difference.\n\n5.0.1 Application\nNow consider how the regression discontinuity design (RDD) method can be applied to our School Feeding Program (SFP). After doing some more investigation into the design of SFP, you find that in addition to randomly selecting treatment villages, the authorities targeted the program to low-income households using the national poverty line. The poverty line is based on a poverty index that assigns each household in the country a score between 20 and 100 based on its assets, housing conditions, and socio demographic structure. The poverty line has been officially set at 58. This means that all households with a score of 58 or below are classified as poor, and all households with a score of more than 58 are considered to be non-poor. Even in the treatment villages, only poor households are eligible to enroll in SFP. Your data set includes information on both poor and non-poor households in the treatment villages\n\n# Create data subset with only treatment localities\ndf_treat &lt;- trans_df %&gt;%\n  filter(treatment_locality == 1)\n\nBefore carrying out the regression discontinuity design estimations, you decide to check whether there is any evidence of manipulation of the eligibility index. As a first step, you check whether the density of the eligibility index raises any concerns about manipulation of the index. You plot the percentage of schools against the baseline poverty index.\n\nggplot(df_treat, aes(x = poverty_index)) +\n  geom_vline(xintercept = 58) +\n  geom_density() +\n  labs(x = \"Poverty Index\")\n\n\n\n\n\n\n\n\nWe can also conduct a McCrary density test, to examine this more formally.\n\ntest_density &lt;- rdplotdensity(rdd = rddensity(df_treat$poverty_index, c = 58), \n                              X = df_treat$poverty_index, \n                              type = \"both\")\n\n\n\n\n\n\n\n\nThe figures do not indicate any clustering of schools right below the cutoff of 58.\nNext, you check whether households respected their assignment to the treatment and comparison groups on the basis of their eligibility score. You plot participation in the program against the baseline poverty index and find that two years after the start of the pilot, only households with a score of 58 or below (that is, to the left of the poverty line) have been allowed to enroll in SFP. In addition, all of the eligible households enrolled in SFP. In other words, you find full compliance and have a “sharp” RDD.\n\nggplot(df_treat, aes(y = enrolled, x = poverty_index)) +\n  geom_vline(xintercept = 58) +\n  geom_point() +\n  labs(x = \"Poverty Index\", y = \"Enrolled\")\n\n\n\n\n\n\n\n\nYou now proceed to apply the RDD method to compute the impact of the program. Using follow-up data, you again plot the relationship between the scores on the poverty index and predicted attendance rates and find the relation illustrated in the figure below. In the relationship between the poverty index and the predicted attendance rates, you find a clear break, or discontinuity, at the poverty line (58).\n\ndf_treat %&gt;%\n  filter(round == 1) %&gt;%\n  mutate(enrolled_lab = ifelse(enrolled == 1, \"Enrolled\", \"Not Enrolled\")) %&gt;%\n  ggplot(aes(x = poverty_index, y = attendance_rate,\n             group = enrolled_lab, colour = enrolled_lab, fill = enrolled_lab)) +\n  geom_point(alpha = 0.03) +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"Poverty Index\", y = \"Attendance_rate\") +\n  scale_colour_viridis_d(\"Enrollment:\", end = 0.7) +\n  scale_fill_viridis_d(\"Enrollment:\", end = 0.7) +\n  theme(legend.position=\"bottom\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe discontinuity reflects an increase in attendance rates for those schools eligible to receive the program. Given that schools on both sides of the cutoff score of 58 are very similar, the plausible explanation for the different level of attendance rates is that one group of schools was eligible to enroll in the program and the other was not. You estimate this difference through a regression with the findings shown in the following table.\n\ndf_treat &lt;- df_treat %&gt;%\n  mutate(poverty_index_c0 = poverty_index - 58)\n\nout_rdd &lt;- lm_robust(attendance_rate ~ poverty_index_c0 * enrolled + \n                       age_hh + age_sp + educ_hh + educ_sp + \n                       female_hh + indigenous + hhsize + dirtfloor + \n                       bathroom + land + school_distance,\n                     data = df_treat %&gt;% filter(round == 1))\n\ntbl16 &lt;- tbl_regression(out_rdd, intercept = T) %&gt;%\n  add_glance_source_note(\n    glance_fun = broom::glance, # Extract model summary\n    include = c(\"adj.r.squared\", \"r.squared\", \"nobs\") # Add Adjusted R-squared\n  )\n\nNote: We could also estimate the effect of the program in the following ways\nEstimating the effect of the program on health expenditures again using regression, but include an interaction with a cubic polynomial of the running variable.\n\nout_rdd_cubic &lt;- lm_robust(attendance_rate ~ enrolled * poverty_index_c0 +\n                             enrolled * I(poverty_index_c0^2) + \n                             enrolled * I(poverty_index_c0^3) +\n                             age_hh + age_sp + educ_hh + educ_sp + \n                             female_hh + indigenous + hhsize + dirtfloor +\n                             bathroom + land + school_distance,\n                           data = df_treat %&gt;% filter(round == 1))\ntbl_cubic &lt;- tbl_regression(out_rdd_cubic, intercept = T) %&gt;% \n  add_glance_source_note(\n    glance_fun = broom::glance, # Extract model summary\n    include = c(\"adj.r.squared\", \"r.squared\", \"nobs\") # Add Adjusted R-squared\n  )\n\nEstimating the effect of the program on attendance rates again using regression, but only including observations 5 points above or below the cutoff of 58.\n\nout_rdd5 &lt;- lm_robust(attendance_rate ~ enrolled * poverty_index_c0 + \n                        age_hh + age_sp + educ_hh + educ_sp + \n                        female_hh + indigenous + hhsize + dirtfloor + \n                        bathroom + land + school_distance,\n                      data = df_treat %&gt;% filter(round == 1 &\n                                                   abs(poverty_index_c0) &lt;=5))\n\ntbl_rdd5 &lt;- tbl_regression(out_rdd5, intercept = T) %&gt;% \n  add_glance_source_note(\n    glance_fun = broom::glance, # Extract model summary\n    include = c(\"adj.r.squared\", \"r.squared\", \"nobs\") # Add Adjusted R-squared\n  )\n\n\ntbl_merge_rdd_all &lt;-\n  tbl_merge(\n    tbls = list( tbl16,tbl_cubic, tbl_rdd5),\n    tab_spanner = c(\"**Linear**\", \"**Cubic**\", \"**5 Point Window**\")\n  )\n\ntbl_merge_rdd_all\n\n\n\n\n  \n    \n      Characteristic\n      \n        Linear\n      \n      \n        Cubic\n      \n      \n        5 Point Window\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n75\n73, 77\n&lt;0.001\n75\n73, 77\n&lt;0.001\n75\n72, 77\n&lt;0.001\n    poverty_index_c0\n-0.15\n-0.22, -0.08\n&lt;0.001\n-0.28\n-0.64, 0.08\n0.13\n-0.35\n-0.75, 0.06\n0.094\n    enrolled\n7.7\n7.0, 8.5\n&lt;0.001\n7.6\n6.4, 8.9\n&lt;0.001\n7.3\n5.9, 8.8\n&lt;0.001\n    age_hh\n-0.08\n-0.11, -0.04\n&lt;0.001\n-0.08\n-0.11, -0.04\n&lt;0.001\n-0.05\n-0.10, -0.01\n0.024\n    age_sp\n0.05\n0.01, 0.09\n0.016\n0.05\n0.01, 0.09\n0.016\n0.02\n-0.03, 0.07\n0.5\n    educ_hh\n0.06\n-0.04, 0.16\n0.2\n0.06\n-0.04, 0.16\n0.2\n0.05\n-0.10, 0.21\n0.5\n    educ_sp\n0.12\n0.01, 0.23\n0.034\n0.12\n0.01, 0.23\n0.038\n0.18\n0.01, 0.34\n0.037\n    female_hh\n-0.49\n-1.5, 0.49\n0.3\n-0.48\n-1.5, 0.50\n0.3\n-0.73\n-2.1, 0.65\n0.3\n    indigenous\n1.6\n1.1, 2.1\n&lt;0.001\n1.6\n1.1, 2.1\n&lt;0.001\n1.7\n0.88, 2.6\n&lt;0.001\n    hhsize\n1.7\n1.6, 1.8\n&lt;0.001\n1.7\n1.6, 1.8\n&lt;0.001\n1.7\n1.6, 1.9\n&lt;0.001\n    dirtfloor\n1.5\n0.99, 2.0\n&lt;0.001\n1.5\n1.0, 2.0\n&lt;0.001\n1.6\n0.83, 2.5\n&lt;0.001\n    bathroom\n-0.49\n-0.94, -0.03\n0.036\n-0.49\n-0.95, -0.03\n0.036\n-0.09\n-0.86, 0.68\n0.8\n    land\n0.04\n-0.03, 0.11\n0.3\n0.04\n-0.03, 0.11\n0.2\n0.01\n-0.11, 0.14\n0.8\n    school_distance\n0.00\n0.00, 0.01\n0.11\n0.00\n0.00, 0.01\n0.11\n0.01\n-0.01, 0.02\n0.3\n    poverty_index_c0 * enrolled\n0.17\n0.09, 0.25\n&lt;0.001\n\n\n\n\n\n\n    I(poverty_index_c0^2)\n\n\n\n0.01\n-0.02, 0.04\n0.4\n\n\n\n    I(poverty_index_c0^3)\n\n\n\n0.00\n0.00, 0.00\n0.4\n\n\n\n    enrolled * poverty_index_c0\n\n\n\n0.33\n-0.10, 0.77\n0.13\n0.36\n-0.16, 0.87\n0.2\n    enrolled * I(poverty_index_c0^2)\n\n\n\n-0.01\n-0.05, 0.02\n0.5\n\n\n\n    enrolled * I(poverty_index_c0^3)\n\n\n\n0.00\n0.00, 0.00\n0.6\n\n\n\n  \n  \n    \n      Adjusted R² = 0.457; R² = 0.458; No. Obs. = 4,960\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nCombining all these results together we see a consistent effect of the program.\n\n\n5.0.2 Results\nIs the result of the RDD analysis valid for all schools in the program?\nNo, the RDD estimates represent the effects for schools with attendance rates very close to the defined eligibility cutoff. Intuitively, this is the region where schools just eligible for the program and those just ineligible have the most similar baseline characteristics and can be meaningfully compared.\nCompared with the impact estimated with the randomized assignment method, what does this result say about schools with poverty index just below the cutoff?\nThis result indicates that schools with poverty index just below the eligibility threshold experience a smaller increase in attendance rates than the average eligible school. Specifically, schools just under the cutoff score experience an increase of 7.3 percentage points in attendance rates, which is slightly less than the average improvement observed with the randomized assignment method.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Discontinuity Designs</span>"
    ]
  },
  {
    "objectID": "Chapter6.html",
    "href": "Chapter6.html",
    "title": "6  Difference-in-Differences",
    "section": "",
    "text": "The Difference-in-Differences (DiD) method is a widely used statistical approach to evaluate the causal effect of an intervention by comparing changes over time between a treatment group and a control group. In our School Feeding Program (SFP), this technique can help us measure how the introduction of the program influenced student attendance rates by comparing schools that implemented the program (treatment group) with those that did not (control group), both before and after the program began.\nThis approach leverages the assumption that, without the intervention, both groups would have followed similar trends over time. By examining the difference in attendance rate changes between these groups, we isolate the program’s impact while controlling for underlying trends that affect all schools.\nIn this scenario, you have two rounds of data on two groups of schools: one group that enrolled in the program, and another that did not. Remembering the case of the enrolled and non- enrolled groups, you realize that you cannot simply compare the average attendance rates of the two groups because of selection bias. Because you have data for two periods for each school in the sample, you can use those data to solve some of these challenges by comparing the change in attendance rates for the two groups, assuming that the change in the attendance rates of the non-enrolled group reflects what would have happened to the attendance of the enrolled group in the absence of the program. Note that it does not matter which way you calculate the double difference.\n\nlibrary(tidyverse)\nlibrary(estimatr)\nout_did &lt;- lm_robust(attendance_rate ~ round * enrolled, \n                     data = trans_df %&gt;% filter(treatment_locality == 1),\n                     clusters = locality_identifier)\n\nout_did_wcov &lt;- lm_robust(attendance_rate ~ round * enrolled +\n                            age_hh + age_sp + educ_hh + educ_sp + \n                            female_hh + indigenous + hhsize + dirtfloor + \n                            bathroom + land + school_distance, \n                          data = trans_df %&gt;% filter(treatment_locality == 1),\n                          clusters = locality_identifier)\n\ntbl_did &lt;- tbl_regression(out_did, intercept = T) %&gt;% \n  add_glance_source_note(\n    glance_fun = broom::glance, # Extract model summary\n    include = c(\"adj.r.squared\", \"r.squared\", \"nobs\") # Add Adjusted R-squared\n  )\n\ntbl_did_wcov &lt;- tbl_regression(out_did_wcov, intercept = T) %&gt;% \n  add_glance_source_note(\n    glance_fun = broom::glance, # Extract model summary\n    include = c(\"adj.r.squared\", \"r.squared\", \"nobs\") # Add Adjusted R-squared\n  )\n\n\ntbl_merge_did &lt;-\n  tbl_merge(\n    tbls = list( tbl_did, tbl_did_wcov),\n    tab_spanner = c(\"**No Covariate Adjustment**\", \"**With Covariate Adjustment**\")\n  )\n\ntbl_merge_did\n\n\n\n\n  \n    \n      Characteristic\n      \n        No Covariate Adjustment\n      \n      \n        With Covariate Adjustment\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n82\n82, 83\n&lt;0.001\n77\n76, 78\n&lt;0.001\n    round\n-1.3\n-1.9, -0.68\n&lt;0.001\n-1.2\n-1.9, -0.62\n&lt;0.001\n    enrolled\n5.4\n5.1, 5.7\n&lt;0.001\n1.3\n1.1, 1.5\n&lt;0.001\n    round * enrolled\n7.0\n6.4, 7.5\n&lt;0.001\n7.0\n6.4, 7.5\n&lt;0.001\n    age_hh\n\n\n\n-0.07\n-0.09, -0.05\n&lt;0.001\n    age_sp\n\n\n\n0.02\n-0.01, 0.04\n0.14\n    educ_hh\n\n\n\n-0.05\n-0.10, 0.00\n0.046\n    educ_sp\n\n\n\n0.07\n0.01, 0.12\n0.030\n    female_hh\n\n\n\n-0.94\n-1.5, -0.40\n0.001\n    indigenous\n\n\n\n2.0\n1.6, 2.4\n&lt;0.001\n    hhsize\n\n\n\n1.7\n1.6, 1.8\n&lt;0.001\n    dirtfloor\n\n\n\n2.0\n1.7, 2.3\n&lt;0.001\n    bathroom\n\n\n\n-0.43\n-0.70, -0.15\n0.003\n    land\n\n\n\n-0.08\n-0.13, -0.03\n0.004\n    school_distance\n\n\n\n0.00\n0.00, 0.01\n0.3\n  \n  \n    \n      Adjusted R² = 0.343; R² = 0.344; No. Obs. = 9,919\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nNext, you estimate the effect using regression analysis. Using a simple linear regression to compute the simple difference-in- differences estimate, you find that the program increased school attendance rate by 7.0. You then refine your analysis by adding additional control variables. In other words, you use a multivariate linear regression that takes into account a host of other factors, and you find the same promotion in school attendance rate.\nWhat are the basic assumptions required to accept this result from difference-in-differences?\nTo accept this result, we assume that there are no differential time varying factors between the two groups other than the program. We assume that the treatment and comparison groups would have equal trends or changes in outcomes in the absence of treatment. While this assumption can’t be tested in the post intervention period, we can compare trends before the intervention starts.\nBased on the result from difference-in-differences, should HISP be scaled up nationally?\nNo, based on this result, the SFP should not be scaled up nationally because it has Increased by less than the $10 threshold level. Taking the estimated impact under random assignment as the “true” impact of the program suggests that the difference in difference estimate may be biased. In fact, in this case, using the nonenrolled households as a comparison group does not accurately represent the counterfactual trend in attendance rates.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Difference-in-Differences</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Practical M&E RCT Evaluations Techniques With R",
    "section": "",
    "text": "Preface\nThe field of Monitoring and Evaluation (M&E) is at the heart of understanding and improving the efficacy of interventions across diverse domains such as health, education, and social welfare. Over the years, the need for robust and context-sensitive evaluation frameworks has grown exponentially, as stakeholders demand tangible and evidence-based outcomes.\nThis book, Practical M&E RCT Evaluations Techniques With R, emerges from my personal journey and professional engagements in the M&E space. Inspired by real-world challenges and successes, it is designed to bridge the gap between theory and practice. By focusing on replicable frameworks, data-driven insights, and the nuances of program contexts, I aim to provide readers with actionable tools for crafting impactful evaluations.\nI extend my heartfelt gratitude to the many organizations, collaborators, and mentors who have shaped my understanding of M&E. I also wish to thank Blessman International for providing the cover image, which captures the essence of the community-driven efforts this book seeks to support.\nWhether you are a student, researcher, program implementer, or policy advocate, this book offers a practical and adaptable approach to designing evaluations that inform and inspire change. Together, let us leverage the power of evaluation to create meaningful and sustainable impacts.\nSincerely,\nVictor Mandela Mokaya,\nStatistics Expert",
    "crumbs": [
      "Preface"
    ]
  }
]